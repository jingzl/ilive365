# 四、大模型原理篇
## 第6章 大模型基础原理
### 6.1 大模型原理及基础概念
#### 6.1.1 大模型的定义
大模型本质上是通过海量数据训练而成的深度神经网络模型，通常拥有海量参数。
人工神经网络的神经元模拟人脑神经元：通过类似树突的机制接收输入信号，经内部处理后，再通过类似轴突的机制输出信号并传递给其他神经元。大模型通过模拟人脑神经元，基于大规模数据训练来调整海量参数，从而推动人工智能发展。现有的大模型技术是实现通用人工智能的一种方式。
图1

#### 6.1.2 大模型基础结构
基于Encoder-Decoder结构，可以将LLM大体分为以下3类：
- 仅包含解码器Decoder模块，主要用于序列生成任务。我们平时非常熟悉的GPT模型的结构就属于此类。
- 仅包含编码器Encoder模块，主要适用于单向任务类场景，如文本分类、情感分析等。这种架构的重点在于理解和编码信息，而不是生成新的文本。这类模型的代表是BERT相关模型，如RoBERTa、ALBERT等。
- 既包含编码器Encoder模块也包含解码器Decoder模块，先理解输入的信息（Encoder模块），然后基于这个理解生成新的相关内容（Decoder模块）。这类模型通常用于序列到序列任务，如机器翻译、对话生成等。
#### 6.1.3 大模型参数训练方式
大模型参数的确定过程大致可以分为4个阶段：预训练（Pretraining）、监督微调（Supervised Fine-tuning，SFT）、构建奖励模型（Reward Modeling，RM）及强化学习（Reinforcement Learning，RL）。
**第一阶段：预训练**
在预训练阶段，模型通过大量没有标注结果的文本信息学习文本的续写能力。
在以往的机器学习预训练中，一般需要将大量的人工标注数据注入模型，以确保模型能够从中学习到准确的知识和信息。然而，获取和标注如此庞大的数据集不仅成本高昂，而且费时费力，几乎是一项不可能完成的任务。BERT模型的出现，为这一难题提供了一种创新的解决方案。BERT模型采用了一种不需要标注数据的训练方法，它通过自监督学习的方式，利用海量无标注文本数据进行训练。与BERT模型不同，GPT采用的是纯语言模型的方法。GPT模仿人类书写习惯，不断预测下一个单词，通过反复训练迭代，将海量数据注入模型参数。
**第二阶段：监督微调**
监督微调是指基于少量人工标注的数据进行微调，数据主要是提示（prompt）和回复（response）对（数量在1万到10万之间）。提示和回复类似于Q（Question）和A（Answer），即问答对。
在监督微调示例中，提示是人类指令，回复是标注员写的针对人类指令的示例回复。标注提示和回复对的过程需要满足“有益、真实、无害”这一约束条件。
监督微调的标注数据中存在这样一个问题，即让标注员编写监督微调的提示和回复对是比较困难的，标注成本极高。ChatGPT引入了RLHF方法。在大模型的训练过程中，将人类反馈纳入奖励模型，以此增强传统强化学习方法，这种方法叫作RLHF。RLHF分为两步，即构建奖励模型和强化学习。
**第三阶段：构建奖励模型**
在大语言模型完成监督微调后，下一阶段是构建一个奖励模型，用于评估问答对的质量。奖励模型源于强化学习中的奖励函数，能对当前的状态给出一个分数，以反映该状态的价值。
奖励模型本质上是一个机器学习模型，用于对大模型输出的结果进行评估，通过对比不同的输出结果，区分其优劣并进行排名。
**第四阶段：强化学习**
机器学习分为有监督学习、无监督学习和强化学习三种类别。有监督学习利用有标签数据来预测结果；无监督学习通过无标签数据寻找数据中隐藏的规律和结构；强化学习也是机器学习的一种范式，不同于有监督学习和无监督学习，强化学习在与环境交互的过程中，借助环境的反馈来调整自身的行为。
强化学习包括智能体和环境两大对象，智能体是算法本身，环境是与智能体交互的外部世界。强化学习的本质是解决如下核心问题：智能体在与环境交互的过程中如何通过学习策略实现回报最大化，从而实现特定目标。
图2

RLHF基于奖励模型进行强化学习训练。RLHF训练框架：
图3

强化学习的过程：
1）针对特定的输入文本，使用经过监督微调的模型获得多个输出文本。
2）基于奖励模型对多个输出文本的质量进行打分。
3）基于打分为多个输出文本结果加入权重。
4）将加权结果反向传播，对经过监督微调的模型参数进行调整。
### 6.2 大模型基座Transformer
#### 6.2.1 背景介绍
GPT的全称为Generative Pre-trained Transformer，可见Transformer是GPT模型中的核心架构。当前的主流大模型都使用Transformer作为其基础组件。
#### 6.2.2 Transformer的基本结构
Transformer是通过使用自注意力机制、多头注意力机制（Multi-Head Attention）处理序列数据的深度学习模型。
**1）自注意力机制**
自注意力机制在处理序列数据时，序列中的每个元素都可以与其他元素建立联系，而不仅仅与相邻的元素建立联系。
**2）多头注意力机制**
图4
Transformer架构的基础单元：
图5

### 6.3 扩散模型
以GPT为代表的大语言模型，推动了大模型的飞速发展，然而这些模型仅局限于文本处理单一领域。与之不同的是，扩散模型在图像生成、视频生成等领域展现出了独特的优势和潜力。
#### 6.3.1 扩散模型背景介绍
扩散模型本质上是一种生成模型，为了更好地理解扩散模型，下面先介绍一下生成模型。我们将生成模型分为三大类，分别是生成对抗网络（Generative Adversarial Networks，GAN）、变分自编码器（Variational Auto-Encoder，VAE）、扩散模型。
**1）GAN**
GAN是一种基于相互竞争的博弈思想而设计的生成模型，它通过对抗方式进行学习。
图6
博弈和相互竞争主要体现在GAN包含两个互相竞争的生成器（Generator）和判别器（Discriminator）。
生成器想要骗过判别器，判别器试图不上当。当两组模型不断训练时，生成器不断生成新的结果进行尝试，直到生成器生成的样本看起来与原始样本没有区别。
生成器从某种噪声分布中随机采样作为输入，输出与训练集中真实样本非常相似的生成样本，进行造假。判别器的输入是真实样本或生成样本，其目标是将生成样本与真实样本尽可能地区分出来。生成器和判别器交替运行，相互对抗，从而使各自的能力都得到提升。理想情况下，经过足够次数的对抗之后，判别器无法判断给定样本的真实性，即对于所有样本都输出50%真、50%假的判断。
**2）VAE**
VAE是一种生成模型，它通过捕获给定数据集的潜在概率分布来生成新样本。VAE使用了编码器-解码器结构。编码器将输入数据转换为潜在形式，解码器旨在基于该潜在表示重建原始数据。
图7
**3）扩散模型**
扩散模型首先逐步向输入数据增加高斯噪声，直到将其变为纯高斯噪声z，然后通过对z进行去噪处理，最终生成新的图像。
GAN、VAE、扩散模型之间的主要差异在于建模方式：
图8
扩散模型由前向扩散和反向扩散过程组成。前向扩散是一个马尔可夫链，它逐渐向输入数据中添加噪声，直到获得白噪声。前向扩散过程不是一个可学习的过程，通常需要 1000 个步骤。反向扩散过程旨在将正向过程反向，从而逐步去除噪声以恢复原始数据。反向扩散过程是使用可训练的神经网络实现的。
#### 6.3.2 扩散模型定义
扩散模型的思想源自物理扩散现象。从一个最终结构被完全破坏的分布，重建出原始的数据分布，这就是扩散模型的基本思想。扩散模型是一种生成模型，它通过模拟随机扩散过程，逐渐将随机噪声转变为目标数据分布，从而生成新的数据样本。简单来说，扩散模型就分为两个过程：“加噪”和“去噪”（也称前向过程和逆向过程）。
**加噪过程**：如图所示，不断向输入数据中加入噪声，直到其变成纯高斯噪声。这一过程是连续的，每一时刻都会基于前一时刻的结果继续添加噪声。
图9
**去噪过程**：如图所示，以一个纯高斯噪声作为开始，逐步地去除噪声，最终得到一个符合训练数据分布的图片。
图10
#### 6.3.3 扩散模型实现文生图
文生图模型基本上分为三个核心模型，即Text Encoder、Diffusion、Image Decoder，这三个模型通常是各自分开训练的。
**Text Encoder模型**一般单独训练，可以通过经典的CLIP模型来实现。实际上，CLIP模型是使用从网络上抓取的图像及其文字说明进行训练的。CLIP模型结合了图像编码器和文本编码器，它的训练过程可以简化为给图片加上文字说明。首先分别使用图像和文本编码器对图片和文本进行编码，然后计算编码后图像和文字的余弦相似度来评估它们是否匹配。最开始训练时，相似度会很低。接着计算损失并更新模型参数，逐步优化图片和文字embedding。通过在训练集上反复训练，最终得到较为准确的文字embedding和图片embedding。
**Image Decoder模型**主要负责将Diffusion模型输出的低分辨率图像或隐变量编码为最终的图像，这部分的训练可以使用非标注数据。
**Diffusion模型**接受Text Encoder的embedding、随机噪声embedding和时间步embedding，并不断执行去噪（Denoise）操作，在完成特定步骤后，Diffusion模型输出隐变量，该隐变量作为Image Decoder的输入。
#### 6.3.4 扩散模型应用场景和前景分析
扩散模型是深度生成模型中目前最先进的技术。扩散模型在图片生成任务中，远远超越了之前的GAN模型，并且在诸多应用领域都有出色的表现。在艺术创作的各个领域，扩散模型都已经得到不错的应用，比如平面设计、影视和动画制作、音乐创作、游戏制作等。
扩散模型的不足：主流的扩散模型目前都需要消耗大量的计算资源，对于需要实时处理或者大规模应用的场景来说，这可能会存在一定的问题。扩散模型在处理没见过的数据时效果不够理想，要想让它们适应特定的领域，可能需要重新训练或者调优。扩散模型可能会从训练数据中学到偏见，所以我们需要确保它们符合伦理规范。
### 6.4 多模态大语言模型
单纯的大语言模型和扩散模型在处理未见过的数据时可能存在局限性。以大语言模型为“大脑”的多模态大语言模型（MLLM）便是这一研究方向上的重要进展。
#### 6.4.1 背景介绍
由大语言模型扩展而来、具备接收与推理多模态信息能力的模型，被称为多模态大语言模型。相较于单模态大语言模型具有以下优势：
1）能够处理用于认知与完成任务的多模态信息。
2）具备更加强大且用户友好的接口。
3）支持更广泛的任务。
多模态大语言模型通常包含5部分，分别是模态编码器、输入投影器、大语言模型骨干网络、输出投影器和模态生成器。多模态大语言模型核心模块的简单描述如下：
1）模态编码器：对不同的模态输入进行编码，得到对应的模态特征。
2）输入投影器：负责将编码后的模态特征投影到文本特征空间。
3）大语言模型骨干网络：以大语言模型作为核心，处理对齐后的特征，进行语义理解、推理和决策，并输出文本信息和信号令牌。
4）输出投影器：将大语言模型骨干网络中的信号令牌映射到多模态特征，以使其可被后续的模态生成器理解。
5）模态生成器：负责生成不同模态的输出，通常采用潜在的扩散模型，将输出投影器映射的特征作为条件输入，以生成多模态内容。
#### 6.4.2 多模态大语言模型训练过程
主要包括多模态预训练和多模态指令调优两个阶段。
1）多模态大语言模型预训练
预训练是多模态训练过程的第一阶段，其核心目标是将不同的模态对齐并让模型学习多模态知识，以便理解并整合不同模态的信息。
2）多模态大语言模型指令调优
指令是任务描述。指令调优是一种微调预训练大语言模型的技术，它通过在一组以指令格式组织的数据集上进行额外的训练，来提高模型对未见过的任务的泛化能力。
多模态大语言模型指令调优阶段，通常使用一组指令格式的数据集对预训练的多模态大语言模型进行微调。这个阶段包括SFT和RLHF，旨在更好地与人类意图保持一致，并增强多模态模型的交互能力。
#### 6.4.3 多模态大语言模型评估
评估是开发多模态大语言模型的重要环节，因为它不仅为模型优化提供了反馈，还有助于比较不同模型的性能。
与传统多模态模型不同，多模态大语言模型需要新的评估策略。根据问题类型，多模态大语言模型的评估大致可以分为两类：封闭集问题和开放集问题。
封闭集问题指答案选项是预定义且有限的问题类型。评估通常在特定任务的数据集上进行。比如一个封闭集问题的评估数据集：**ScienceQA**。
关于开放集问题的评估，我们介绍一个多模态生成AI排名开放平台：**GenAI-Arena**。
### 6.5 推理大模型
早期大语言模型虽在文本生成和理解上表现出色，但在复杂逻辑推理、数学证明、跨领域知识整合等任务中表现不稳定，这本质上是因为大语言模型的推理能力相对较弱。2024年9月，OpenAI发布了o1模型，作为首个以“推理”为核心定位的大模型，o1的发布标志着大模型的技术探索转向更为复杂的“推理能力”研究。2025年初，DeepSeek公司开源了DeepSeek-R1模型，将大模型推理能力的研究推向高潮，并引发了全球范围内关于大模型发展及其相关问题的讨论。
#### 6.5.1 背景介绍
尽管模型o1和模型o3的发布在科技行业引起了轰动，但OpenAI延续了以往的风格，只提供闭源模型调用，并未开源算法和模型。当研究者纷纷猜测OpenAI推理能力的实现路径时，来自我国的DeepSeek公司开源了DeepSeek-R1模型，并在多个基准测试上对模型进行了评估，包括知识、推理、编码等任务。
在各类基准测试（如AIME、GPQA、MMLU）、编码任务（如Codeforces）和数学任务（如MATH-500）上，DeepSeek-R1均展现出了优异的表现，与OpenAI的模型o1-1217不相上下。此外，DeepSeek-R1蒸馏后的小模型也超越了部分基线模型。
#### 6.5.2 DeepSeek-R1核心原理
基本原理涉及强化学习、思维链（Chain of Thought, CoT）提示、模型蒸馏（Model Distillation）等多个方面。
1）强化学习
采用组相对策略优化（Group Relative Policy Optimization，GRPO）算法进行强化学习训练。
2）思维链提示
思维链提示工程方法通过将复杂问题拆解成一系列逻辑步骤，引导模型按步骤进行推理，模仿人类的推理过程。
3）模型蒸馏
模型蒸馏是一种将知识从较大的模型（教师模型）转移到较小的模型（学生模型）的技术，旨在降低计算资源需求的同时，保留并提升模型的推理能力。
#### 6.5.3 DeepSeek-R1训练过程
DeepSeek-R1的训练过程精心设计了4个主要阶段：
**阶段1：冷启动**
这一阶段的训练过程本质上就是通过冷启动数据对DeepSeek-V3进行监督微调训练（SFT），从而得到微调后的模型（这里简称其为DeepSeek-R1-SFT1）。
图11
**阶段2：推理导向的强化学习**
以DeepSeek-R1-SFT1为基础，采用GRPO强化学习方法进行训练，最终得到DeepSeek-R1-RL1模型。
图12
**阶段3：拒绝采样和监督微调**
在推理导向的强化学习训练达到收敛后，利用此时的模型检查点，通过拒绝采样的方法广泛收集推理数据。
图13
**阶段4：全场景强化学习**
为了使模型在更多场景下更加符合人类的偏好，DeepSeek团队进行二次RL训练。对于推理数据，继续沿用DeepSeek-R1-Zero中基于规则的奖励机制，引导模型在数学、代码和逻辑推理等领域进行深入学习；对于一般数据，借助奖励模型来捕捉人类在复杂和细微场景下的偏好，使模型能够更好地适应这些场景。
图14
#### 6.5.4 DeepSeek-R1历史意义
训练方法创新突破、性能表现卓越、模型蒸馏技术推进、开源推动研究发展。

## （未完待续）
---