图

## 基础概念
- **人工智能（Artificial Intelligence，AI）**：一种模拟人类智能行为和思维的计算机技术，使计算机能够执行诸如学习、推理、感知、语言理解等智能任务。
- **AI生成内容（AI-Generated Content，AIGC）**：指通过人工智能技术生成的各种内容，包括文本、图像、音频、视频等。
- **最先进技术（State of The Art，SOTA）**：指在某个领域内当前最先进、性能最好的技术或模型。
- **机器学习（Machine Learning，ML）**：AI的一个子领域，让计算机通过数据学习规律和模式，从而能够进行预测或决策，而无需进行明确的编程。
- **深度学习（Deep Learning，DL）**：机器学习的一个分支，基于人工神经网络（特别是深层神经网络）进行学习和模式识别，如图像识别、语音识别等。
- **神经网络（Neural Network，NN）**：模拟人脑神经元结构和功能的计算模型，由大量节点（神经元）和连接（突触）组成，用于处理和分析数据。
- **数据挖掘（Data Mining，DM）**：从大量数据中发现有用信息和模式的过程，与机器学习密切相关，但更侧重于商业和实际应用中的数据价值挖掘。
- **自然语言处理（Natural Language Processing，NLP）**：使计算机能够理解、生成和处理人类语言的技术，包括语音识别、文本分析、机器翻译等。
- **计算机视觉（Computer Vision，CV）**：使计算机能够像人类一样理解和解释视觉信息（如图像和视频）的技术，用于目标检测、图像识别、视频分析等。
- **算法（Algorithm）**：一系列定义明确的计算步骤，用于解决特定问题或执行特定任务。
- **拟人化（Anthropomorphize）**：将人类特征归因于非人类实体，如将情感或意图归因于AI系统。
- **人工通用智能（Artificial General Intelligence，AGI）**：一种具有广泛智能的人工智能，能够执行任何智能人类能够执行的智力任务。
- **人工超级智能（Artificial Superintelligence，ASI）**：超越人类智能的人工智能，理论上能够超越人类在所有领域的智力。
- **生成式AI（Generative AI）**：能够生成新内容（如文本、图像）的AI系统，通常基于深度学习模型。
- **生成对抗网络（Generative Adversarial Network，GAN）**：由生成器和判别器组成的模型，用于生成新的、逼真的数据样本。
- **生成预训练变换器（GPT）**：一种用于自然语言处理任务的深度学习模型。
- **大数据（Big Data）**：用于训练 AI 模型的大规模数据集。
- **真实值（Ground truth）**：指实际的、正确的答案或结果，用于评估模型预测的准确性。
- **大型语言模型（Large Language Model，LLM）**：具有大量参数的语言模型，能够生成高质量的文本内容。
- **视觉语言模型（Visual Language Model，VLM）**：能够理解和生成视觉与语言相关联的内容的模型。
- **稳定扩散（Stable Diffusion，SD）**：一种开源的文生图大模型，由美国Stability AI公司开发。
- **Midjourney（MJ）**：另一种流行的文生图大模型，与Stable Diffusion具有相似的功能。
- **多模态（Multimodal）**：指能够处理和理解多种类型数据（如文本、图像、音频、视频等）的模型或技术。
- **大模型幻觉（Hallucination）**：大模型生成的虚假或不正确的信息，可能导致模型输出与事实不符。
- **提示词（prompt）**：用于引导大模型生成特定内容的输入信息，包括用户提示词和系统提示词。
- **提示词工程（Prompt Engineering）**：设计和优化提示词以提高大模型性能的过程。
- **指令微调（Instruction prompting）**：通过指令和示例对模型进行微调，以使其更好地理解人类指令。
- **检索增强生成（Retrieval-Augmented Generation，RAG）**：结合检索和生成的模型架构，用于知识密集型任务。
- **AI智能体平台（AI Agent 平台）**：支持用户快速搭建chatbot的平台，提供零代码或低代码的开发方式。
- **AI智能体应用（AI Agent 应用）**：基于大模型技术搭建的智能体应用，通常以chatbot形式与用户交互。
- **对话机器人（chatbot）**：能够与用户进行对话的智能助手，基于大模型技术的chatbot更为智能。
- **AI原生（AI Native）**：从设计之初就集成人工智能技术的应用程序，以增强用户体验和性能。
- **专家混合模型（Mixture of Experts，MoE）**：一种模型架构，通过多个专家网络的组合提高模型性能。
- **专家交叉注意力机制（Cross-attention of Experts，CoE）**：专家混合模型中的注意力机制。
- **语音识别技术（Automatic Speech Recognition，ASR）**：将语音转换为文本的技术。
- **文本合成语音技术（Text-to-Speech，TTS）**：将文本转换为语音的技术。

## 理论
- **信息熵（Information Entropy）**：度量不确定性或信息含量的单位，常用于信息论和统计学中。
- **贝叶斯定理（Bayes’ Theorem）**：描述条件概率的关系，是贝叶斯统计的基础。
- **信息增益（Information Gain）**：用于特征选择的度量标准，表示特征对分类的贡献程度。
- **马尔可夫决策过程（Markov Decision Process, MDP）**：强化学习中的框架，定义了一个决策问题的数学模型。
- **博弈论（Game Theory）**：研究战略情况下的决策制定，适用于经济学、政治学等领域。
- **统计显著性（Statistical Significance）**：指数据结果是否可能由随机变化引起，用于评估假设检验的结果。
- **因果推理（Causal Inference）**：确定因果关系的逻辑和方法，常用于社会科学和医学研究。
- **不确定性量化（Quantification of Uncertainty）**：表征和管理不确定性的方法，涉及概率分布和区间估计。
- **认知科学（Cognitive Science）**：研究人类思维和知觉的跨学科领域，涵盖心理学、计算机科学等多个学科。
- **复杂系统理论（Complex Systems Theory）**：研究高度动态且相互关联的系统，如生态系统、社会网络等。

## 机器学习与数据科学
- **监督学习（Supervised Learning）**：机器学习的一种方式，使用带有标签的数据进行训练，模型通过学习输入与输出之间的映射关系来进行预测。
- **无监督学习（Unsupervised Learning）**：机器学习的一种方式，使用未标记的数据进行训练，模型通过发现数据中的内在结构和模式来进行聚类或降维等任务。
- **强化学习（Reinforcement Learning，RL）**：机器学习的一种方式，通过与环境的交互，根据获得的奖励或惩罚来学习最优的行为策略。
- **半监督学习（Semi-Supervised Learning）**：结合监督学习和无监督学习，使用少量标记数据和大量未标记数据。
- **迁移学习（Transfer Learning）**：将一个领域的知识转移到另一个相关领域，加速学习或提高性能。
- **特征工程（Feature Engineering）**：选择和构造有助于模型预测的输入变量，包括特征选择、构造、缩放等。
- **数据预处理（Data Preprocessing）**：清洗、转换和标准化数据，包括去除噪声、填补缺失值、数据标准化等。
- **降维（Dimensionality Reduction）**：减少数据特征数量，同时尽量保留重要信息，常用方法包括 PCA、t-SNE 等。
- **过拟合（Overfitting）**：模型对训练数据过于敏感，无法泛化到新数据，可通过正则化、早停等手段缓解。
- **欠拟合（Underfitting）**：模型过于简单，无法捕捉数据中的模式，解决方法包括增加模型复杂度或特征工程。
- **交叉验证（Cross Validation）**：评估模型性能的技术，通过将数据集分成训练集和验证集，多次重复训练和验证过程。
- **回归分析（Regression Analysis）**：预测连续值输出的统计方法，常用方法包括线性回归、多元回归等。
- **分类（Classification）**：将输入数据分配到预定义类别中的任务，常用算法包括逻辑回归、支持向量机等。
- **聚类（Clustering）**：将数据点分成多个组，组内成员比组间成员更相似，常用方法包括 K 均值聚类。
- **决策树（Decision Tree）**：树形结构模型，用于分类或回归，每个内部节点表示一个属性上的测试，每个分支代表一个测试结果，每个叶子节点代表一个类别或输出值。
- **随机森林（Random Forest）**：由多个决策树组成的集合模型，通过集成多个弱分类器提高预测的准确性和鲁棒性。
- **支持向量机（Support Vector Machine, SVM）**：用于分类和回归的监督学习模型，试图找到一个超平面来最大化分类间隔。
- **朴素贝叶斯（Naive Bayes）**：基于贝叶斯定理的分类算法，假设特征之间相互独立。
- **逻辑回归（Logistic Regression）**：用于解决二分类问题的概率统计方法，使用 Sigmoid 函数将线性组合的输出转换为概率值。
- **梯度下降（Gradient Descent）**：优化算法，用于最小化损失函数，通过沿负梯度方向逐步更新参数达到最小化损失的目的。
- **对抗性机器学习（Adversarial Machine Learning）**：一种机器学习方法，通过创建对抗性示例来测试和改进模型的鲁棒性。
- **AI分析（AI Analytics）**：利用人工智能技术来分析数据并提取有价值的洞察力。
- **AI助手（AI Assistant）**：人工智能助手，如虚拟助手，可以执行任务、回答问题和提供信息。
- **AI偏见（AI Bias）**：人工智能系统中存在的偏见，通常是由于训练数据的不平衡或算法设计不当。
- **人工智能增强的网络安全（AI-Enhanced Cybersecurity）**：使用人工智能技术来增强网络安全，例如通过检测异常行为和自动化响应。
- **人工智能安全（AI Safety）**：确保人工智能系统在设计和运行时的安全性和可靠性。
- **关联规则学习（Association Rule Learning）**：一种数据挖掘技术，用于发现大型数据集中变量之间的有趣关系。
- **自动化机器学习（Automated Machine Learning，AutoML）**：自动化机器学习流程，包括模型选择、超参数调整和模型验证。
- **增强智能（Augmented Intelligence）**：结合人类智能和人工智能，以提高决策和问题解决能力。
- **自动完成（Auto-complete）**：一种功能，系统预测并完成用户输入的文本。
- **自动分类（Auto Classification）**：自动将数据分类到预定义的类别中。
- **自动语音识别（Automatic Speech Recognition）**：将口语转换为文本的技术。
- **自动内容创造（Automated Content Creation）**：使用算法自动生成内容，如文章、报告或社交媒体帖子。
- **行为分析（Behavioral Analytics）**：分析用户行为数据以了解用户趋势和模式。
- **贝叶斯网络（Bayesian Network）**：一种概率图模型，用于表示变量之间的条件依赖关系。
- **黑盒AI（Black Box AI）**：指那些对其内部工作机制不透明的AI系统，用户无法了解其决策过程。
- **暴力搜索（Brute Force Search）**：一种通过尝试所有可能的解决方案来寻找问题答案的方法。
- **聊天机器人（Chatbot）**：一种通过文本或语音进行交互的软件，模拟与人类用户的对话。
- **聚类（Cluster）**：在数据挖掘中，将相似数据对象分组的过程。
- **认知计算（Cognitive Computing）**：模拟人类认知过程的计算系统，以增强或扩展人类智力。
- **认知科学（Cognitive Science）**：研究认知过程的科学，包括感知、思考、记忆和语言等。
- **综合AI（Composite AI）**：结合多种AI技术，如机器学习、自然语言处理和机器人技术，以解决复杂问题。
- **对话AI（Conversational AI）**：专注于创建能够进行自然对话的AI系统。
- **语料库（Corpus）**：在自然语言处理中，用于训练和测试模型的大型文本数据集。
- **客户情绪分析（Customer Sentiment Analysis）**：分析客户反馈中的情感倾向，以了解客户对产品或服务的看法。
- **数据挖掘（Data Mining）**：从大量数据中提取有用信息和模式的过程。
- **数据集（Dataset）**：一组数据，通常用于分析、研究或机器学习。
- **深度学习（Deep Learning）**：一种机器学习方法，使用多层神经网络来模拟人类学习。
- **深度神经网络（Deep Neural Network）**：一种包含多个隐藏层的神经网络，能够学习复杂的数据模式。
- **人工智能边缘计算（Edge Computing in AI）**：在网络边缘进行数据处理和分析，以减少延迟并提高效率。
- **情感AI（Emotional AI）**：能够识别、解释和模拟人类情感的AI系统。
- **实体注释（Entity Annotation）**：在文本中标记和分类实体（如人名、地点、组织）的过程。
- **实体抽取（Entity Extraction）**：从文本中自动识别实体的过程。
- **集成学习（Ensemble Learning）**：结合多个机器学习模型的预测，以提高整体性能。
- **百亿亿次计算（Exascale Computing）**：指每秒执行数十亿亿次计算的高性能计算系统。
- **可解释AI（Explainable AI）**：能够提供其决策过程和结果解释的AI系统。
- **面部识别（Facial Recognition）**：使用面部特征来识别或验证个人身份的技术。
- **特征工程（Feature Engineering）**：在数据挖掘中，选择、构建和优化输入变量（特征）的过程，以提高模型的性能。
- **特征提取（Feature Extraction）**：从原始数据中提取关键信息以减少数据维度的过程。
- **前向链结（Forward Chaining）**：一种推理方法，从已知事实出发，逐步推导出新的结论。

## 神经网络与学习
- **神经网络（Neural Networks）**：由大量简单计算单元（神经元）连接而成，用于模拟人类大脑的结构和功能，处理和传递信息。
- **激活函数（Activation Function）**：用于神经网络中，将输入信号转换为输出信号，引入非线性因素，使网络能够学习复杂的模式和特征。
- **双曲正切函数（Hyperbolic Tangent）**：一种常见的激活函数，输出范围在(-1,1)之间，能够将输入值压缩到这个区间。
- **偏置项（Bias Units）**：神经网络中的一个参数，用于调整神经元的激活阈值，增加模型的灵活性和拟合能力。
- **激活值（Activation）**：神经元在接收到输入信号并经过激活函数处理后产生的输出值。
- **前向传播（Forward Propagation）**：神经网络中信号从输入层经过隐藏层向输出层传递的过程，用于计算网络的输出。
- **前馈神经网络（Feedforward Neural Network）**：一种神经网络结构，信息在其中单向流动，不会出现循环或反馈连接。
- **反向传播算法（Backpropagation Algorithm）**：用于训练神经网络，通过计算损失函数对网络参数的梯度，调整参数以最小化损失。
- **（批量）梯度下降法（Batch Gradient Descent）**：一种优化算法，用于最小化损失函数，通过沿着梯度方向更新参数。
- **（整体）代价函数（Overall Cost Function）**：衡量模型预测值与真实值之间的差异，用于评估模型性能并指导参数调整。
- **方差（Squared-Error）**：一种常用的损失函数，计算预测值与真实值之间差值的平方。
- **均方差（Average Sum-of-Squares Error）**：对所有样本的方差求平均，用于衡量模型的整体误差。
- **规则化项（Regularization Term）**：添加到损失函数中的项，用于防止过拟合，提高模型的泛化能力。
- **权重衰减（Weight Decay）**：一种规则化方法，通过在损失函数中添加权重的平方和项，限制权重的大小。
- **贝叶斯规则化方法（Bayesian Regularization Method）**：基于贝叶斯统计的规则化方法，通过引入先验分布来约束模型参数。
- **高斯先验概率（Gaussian Prior）**：假设模型参数服从高斯分布的先验概率，用于贝叶斯规则化中。
- **极大后验估计（MAP）**：一种参数估计方法，通过最大化后验概率来估计模型参数。
- **极大似然估计（Maximum Likelihood Estimation）**：一种统计方法，用于估计模型参数，使得观测数据出现的概率最大。
- **非凸函数（Non-Convex Function）**：指具有多个局部极小值的函数，优化过程中容易陷入局部最优。
- **隐藏层单元（Hidden Layer Units）**：神经网络中位于输入层和输出层之间的神经元，用于提取和转换输入数据的特征。
- **对称失效（Symmetry Breaking）**：指在神经网络初始化过程中，通过打破参数的对称性，使网络能够有效学习。
- **学习速率（Learning Rate）**：控制参数更新步长的超参数，影响模型的收敛速度和效果。
- **假设值（Hypothesis）**：模型对输入数据的预测值。
- **残差（Error Term）**：预测值与真实值之间的差异。
- **加权平均值（Weighted Average）**：考虑不同数据点的重要性，给予不同权重后计算的平均值。
- **阿达马乘积（Hadamard Product）**：两个矩阵对应元素相乘得到的新矩阵。
- **缺位错误（Off-by-One Error）**：指在编程或计算过程中，由于索引或计数错误导致的结果偏差。
- **数值检验（Numerically Checking）**：通过数值方法检查计算结果的正确性。
- **数值舍入误差（Numerical Roundoff Errors）**：由于计算机有限精度表示导致的计算误差。
- **有效数字（Significant Digits）**：表示测量或计算结果的精度。
- **组合扩展（Unrolling）**：将循环结构展开为多个步骤，便于分析和优化。
- **海森矩阵（Hessian Matrix）**：由二阶偏导数组成的方阵，用于优化算法中分析函数的曲率。
- **牛顿法（Newton’s Method）**：一种迭代优化算法，利用一阶和二阶导数信息快速收敛。
- **共轭梯度（Conjugate Gradient）**：一种优化算法，用于求解大规模线性方程组和无约束优化问题。
- **步长值（Step-Size）**：控制每次迭代中参数更新的幅度。
- **自编码算法（Autoencoders）**：一种无监督学习模型，用于学习数据的高效表示和特征提取。
- **稀疏性（Sparsity）**：指数据或模型参数中大部分值为零的特性，有助于降低计算复杂度和提高模型的可解释性。
- **像素灰度值（The Pixel Intensity Value）**：图像中每个像素点的亮度值，通常用0到255之间的整数表示。
- **独立同分布（IID）**：假设数据样本之间相互独立且服从相同的概率分布。
- **主元分析（PCA）**：一种降维技术，通过提取数据的主要特征来降低数据维度，同时保留大部分信息。
- **激活（Active）**：神经元被激活的状态，即输出非零值。
- **抑制（Inactive）**：神经元未被激活的状态，即输出零值。
- **平均活跃度（The Average Activation）**：神经元在所有输入数据上的平均激活值。
- **稀疏性参数（Sparsity Parameter）**：控制模型输出中零值或接近零值的比例，用于稀疏编码和稀疏自编码器。
- **惩罚因子（Penalty Term）**：添加到损失函数中的项，用于规则化或鼓励特定的模型行为。
- **KL 散度（KL Divergence）**：衡量两个概率分布之间的差异，用于稀疏编码中的稀疏性约束。
- **伯努利随机变量（Bernoulli Random Variable）**：只有两种可能取值（0或1）的离散随机变量，用于稀疏自编码器中表示神经元的激活状态。
- **目标函数（The Objective）**：同“(overall) cost function”。
- **梯度验证方法（The Derivative Checking Method）**：用于检查通过反向传播计算的梯度是否正确。
- **可视化（Visualizing）**：将数据、模型结构或中间结果以图形或图像形式展示，便于理解和分析。
- **非线性特征（Non-Linear Feature）**：无法通过线性组合输入变量得到的特征，需要使用非线性变换或模型来提取。
- **平凡解（Trivial Answer）**：指模型输出全为零或恒定值，没有学习到任何有用信息。
- **范数约束（Norm Constrained）**：对模型参数的范数进行限制，防止参数过大或过小。
- **稀疏自编码器（Sparse Autoencoder）**：一种自编码器变体，通过稀疏性约束使隐藏层单元只有少数被激活，从而学习更高效的特征表示。
- **有界范数（Norm Bounded）**：对参数的范数进行限制，使其不超过某个阈值。
- **输入域（Input Domains）**：模型输入数据的范围或空间。
- **矢量化（Vectorization）**：将数据或操作转换为向量形式，提高计算效率和速度。
- **逻辑回归（Logistic Regression）**：一种用于二分类问题的统计学习方法，通过Sigmoid函数将线性组合的输出转换为概率值。
- **批量梯度上升法（Batch Gradient Ascent）**：与梯度下降相反，用于最大化目标函数，每次迭代使用整个数据集计算梯度。
- **截距（Intercept Term）**：线性模型中的常数项，表示当所有输入变量为零时模型的输出值。
- **对数似然函数（The Log Likelihood）**：用于评估模型参数在给定数据下的概率，极大化对数似然函数等价于极大似然估计。
- **导函数（Derivative）**：衡量函数在某一点处的变化率，用于优化算法中计算梯度。
- **梯度（Gradient）**：多变量函数的导数，指向函数增长最快的方向，用于优化算法中指导参数更新。
- **训练样本（Training Examples）**：用于训练模型的数据实例，包含输入特征和对应的目标值。
- **Softmax回归（Softmax Regression）**：一种用于多分类问题的广义线性模型，通过Softmax函数将线性组合的输出转换为概率分布。
- **深度学习（Deep Learning）**：一种基于多层神经网络的学习方法，能够自动学习数据的高层次特征表示。
- **二元分类（Binary Classification）**：将数据分为两个类别的任务。
- **类型标记（Class Labels）**：数据样本所属的类别标签。
- **多元分类（Multi-Class Classification）**：将数据分为多个类别的任务。
- **自我学习/自学习（Self-Taught Learning）**：一种学习方法，利用无标签数据进行预训练，然后在少量有标签数据上进行微调。
- **无监督特征学习（Unsupervised Feature Learning）**：从无标签数据中自动学习有用的特征表示。
- **半监督学习（Semi-Supervised Learning）**：结合有标签数据和无标签数据进行学习，以提高模型性能。
- **深度网络（Deep Networks）**：具有多个隐藏层的神经网络，能够学习更复杂的特征层次结构。
- **微调（Fine-Tune）**：对预训练的模型在特定任务上进行进一步训练，以适应新的数据和任务。
- **预训练（Pre-Training）**：在大规模无标签数据上进行初步训练，为后续的微调做准备。
- **非线性变换（Non-Linear Transformation）**：通过非线性函数对输入数据进行转换，使模型能够学习更复杂的模式。
- **简洁地表达（Represent Compactly）**：通过学习高效的特征表示，用较少的参数或维度描述数据。
- **“部分-整体”的分解（Part-Whole Decompositions）**：将复杂对象分解为多个部分，并学习各部分之间的关系和组合方式。
- **目标的部件（Parts of Objects）**：构成复杂对象的各个组成部分，如图像中的边缘、纹理等。
- **高度非凸的优化问题（Highly Non-Convex Optimization Problem）**：具有多个局部极小值的优化问题，难以找到全局最优解。
- **梯度的弥散（Diffusion of Gradients）**：指梯度在反向传播过程中逐渐减小甚至消失的现象，导致深层网络难以训练。
- **逐层贪婪训练方法（Greedy Layer-Wise Training）**：逐层训练深度网络，每层在固定下层参数的情况下进行优化，逐步构建整个网络。
- **栈式自编码神经网络（Stacked Autoencoder）**：由多个自编码器堆叠而成的深度网络，能够逐层学习数据的高层次特征。
- **原始输入（Raw Inputs）**：未经过任何处理或转换的原始数据，如原始像素值、文本字符串等。
- **层次型分组（Hierarchical Grouping）**：将数据按照层次结构进行分组和组织，有助于模型学习不同层次的特征和模式。
- **一阶特征（First-Order Features）**：直接从原始数据中提取的低层次特征，如像素值、词频等。
- **二阶特征（Second-Order Features）**：通过对一阶特征进行组合或变换得到的特征，能够捕捉数据中的更高层次信息。
- **更高阶特征（Higher-Order Features）**：通过进一步组合和变换二阶特征得到的特征，用于描述更复杂的模式和关系。
- **线性解码器（Linear Decoders）**：使用线性变换将编码后的特征还原为原始数据或目标输出的模型组件。
- **输入层（Input Layer）**：神经网络的第一层，接收外部输入数据。
- **隐含层（Hidden Layer）**：位于输入层和输出层之间的网络层，用于对输入数据进行非线性变换和特征提取。
- **输出层（Output Layer）**：神经网络的最后一层，产生模型的最终输出结果。
- **神经元（Neuron）**：神经网络的基本计算单元，模拟生物神经元的功能，接收输入信号、进行计算并产生输出。
- **鲁棒（Robust）**：模型对输入数据的微小变化或噪声具有较强的抵抗能力，不会导致输出结果的大幅波动。
- **S型激励函数（Sigmoid Activation Function）**：一种常见的激活函数，输出范围在(0,1)之间，呈S型曲线。
- **tanh激励函数（Tanh Function）**：同“hyperbolic tangent”。
- **线性激励函数（Linear Activation Function）**：输出与输入成线性关系的激活函数，没有引入非线性因素。
- **恒等激励函数（Identity Activation Function）**：输出等于输入的激活函数，常用于输出层需要直接预测连续值的情况。
- **隐单元（Hidden Unit）**：同“hidden (layer) units”。
- **权重（Weight）**：神经网络中连接两个神经元的参数，表示输入信号在传递过程中的重要性和影响程度。
- **偏差项（Error Term）**：同“error term”。
- **全连接网络（Full Connected Networks）**：网络中每一层的每个神经元都与下一层的所有神经元相连，能够学习复杂的全局特征关系。
- **部分联通网络（Locally Connected Networks）**：网络中每个神经元只与下一层的部分神经元相连，减少参数数量，适用于局部特征提取。
- **连接区域（Contiguous Groups）**：连续的区域或块，在图像处理中常指相邻的像素区域。
- **视觉皮层（Visual Cortex）**：大脑中负责处理视觉信息的区域，为设计人工神经网络提供了生物启发。
- **卷积（Convolution）**：一种数学运算，用于提取数据的局部特征，特别是在图像处理和卷积神经网络中广泛应用。
- **固有特征（Stationary）**：指数据的统计特性在时间或空间上保持不变，便于模型的学习和泛化。
- **池化（Pool）**：一种降维操作，通过取局部区域的最大值、平均值等统计量来减少数据的维度和计算量，同时保留重要特征。
- **特征（Features）**：数据中用于表示和区分不同样本的属性或变量，是机器学习和模式识别的核心概念。
- **样例（Example）**：同“training examples”。
- **平移不变性（Translation Invariant）**：模型对输入数据的平移变换具有不变性，即无论对象在图像中的位置如何，模型都能识别它。
- **物体检测（Object Detection）**：从图像或视频中识别和定位感兴趣物体的任务，广泛应用于安防、自动驾驶等领域。
- **直流分量（DC Component）**：信号或图像的平均亮度值，对应于频率为零的分量。
- **局部均值消减（Local Mean Subtraction）**：对数据进行局部区域的均值减去操作，用于数据的标准化和归一化。
- **消减归一化（Sparse Autoencoder）**：可能指通过稀疏自编码器实现数据的归一化处理。
- **缩放（Rescaling）**：将数据按比例调整到特定范围，如将图像像素值从0-255缩放到0-1。
- **逐样本均值消减（Per-Example Mean Subtraction）**：对每个样本单独进行均值减去操作，消除样本间的亮度差异。
- **特征标准化（Feature Standardization）**：将特征值调整到具有零均值和单位方差的分布，提高模型的稳定性和收敛速度。
- **平稳（Stationary）**：同“stationary”。
- **零均值化（Zero-Mean）**：同“zero-mean”。
- **低通滤波（Low-Pass Filtering）**：允许低频信号通过而衰减高频信号的滤波过程，用于去除噪声和细节信息。
- **基于重构的模型（Reconstruction Based Models）**：通过学习数据的重构表示来进行特征提取和模式识别。
- **受限Boltzman机（RBMs）**：一种生成模型，用于学习数据的概率分布，广泛应用于特征学习和降维。
- **k-均值（k-Means）**：一种聚类算法，将数据分为k个簇，每个簇由其质心表示。
- **长尾（Long Tail）**：指数据分布中尾部较长，包含大量低频但多样化的数据点，常见于文本、图像等自然数据中。
- **损失函数（Loss Function）**：同“(overall) cost function”。
- **正交化（Orthogonalization）**：使向量或矩阵满足正交条件的过程，用于消除特征之间的相关性。
- **稀疏编码（Sparse Coding）**：一种特征学习方法，通过稀疏性约束使数据表示为少量基向量的线性组合。
- **无监督学习（Unsupervised Method）**：同上。
- **超完备基（Over-Complete Bases）**：基向量的数量多于数据维度的基集合，能够更灵活地表示数据，但可能导致过拟合。
- **退化（Degeneracy）**：指多个不同的输入或参数组合导致相同输出或结果的情况，可能影响模型的唯一性和稳定性。
- **重构项（Reconstruction Term）**：衡量重构数据与原始数据之间差异的项，用于自编码器等模型的损失函数中。
- **稀疏惩罚项（Sparsity Penalty）**：同“sparsity penalty”。
- **范式（Norm）**：衡量向量或矩阵大小的函数，如L1范数、L2范数等，用于规则化和约束模型参数。
- **生成模型（Generative Model）**：能够生成新数据样本的模型，学习数据的联合概率分布。
- **线性叠加（Linear Superposition）**：多个信号或函数的线性组合，用于表示复杂的数据和模式。
- **加性噪声（Additive Noise）**：直接加到信号上的噪声，如高斯噪声、椒盐噪声等。
- **特征基向量（Basis Feature Vectors）**：用于表示数据的基向量，通过线性组合这些基向量可以重构原始数据。
- **经验分布函数（The Empirical Distribution）**：根据样本数据估计的概率分布，反映数据的统计特性。
- **对数似然函数（The Log-Likelihood）**：同上。
- **高斯白噪音（Gaussian White Noise）**：均值为零、方差恒定的高斯分布噪声，且不同时间点的噪声相互独立。
- **先验分布（The Prior Distribution）**：对模型参数的概率分布的先验假设，用于贝叶斯统计和规则化中。
- **先验概率（Prior Probability）**：同“the prior distribution”。
- **源特征（Source Features）**：数据的原始特征或基本组成单元，如图像的像素、文本的单词等。
- **能量函数（The Energy Function）**：用于描述系统状态的能量值，能量越低表示系统越稳定，常用于生成模型和优化问题中。
- **正则化（Regularized）**：同“regularization”。
- **最小二乘法（Least Squares）**：一种参数估计方法，通过最小化观测值与模型预测值之间的平方差来估计模型参数。
- **凸优化软件（Convex Optimization Software）**：用于求解凸优化问题的软件工具，保证找到全局最优解。
- **共轭梯度法（Conjugate Gradient Methods）**：同“conjugate gradient”。
- **二次约束（Quadratic Constraints）**：对二次函数形式的约束条件，用于优化问题中限制变量的取值范围。
- **拉格朗日对偶函数（The Lagrange Dual）**：用于将带有约束的优化问题转化为无约束的对偶问题，便于求解。
- **前馈结构算法（Feedforward Architectures）**：同“feedforward neural network”。
- **独立成分分析（Independent Component Analysis）**：一种盲源分离技术，用于从混合信号中分离出相互独立的源信号。
- **超完备基（Over-Complete Basis）**：同上。
- **标准正交基（Orthonormal Basis）**：基向量之间正交且单位长度的基集合，便于计算和分析。
- **稀疏惩罚项（Sparsity Penalty）**：同上。
- **不完备基（Under-Complete Basis）**：基向量的数量少于数据维度的基集合，可能导致数据的部分信息丢失。
- **线搜索算法（Line-Search Algorithm）**：用于在优化过程中确定最优步长的算法，常与其他优化方法结合使用。
- **拓扑代价项（Topographic Cost Term）**：用于保持数据拓扑结构的代价项，使模型能够学习数据的空间或结构关系。

## 深度学习与神经网络
- **人工神经网络（Artificial Neural Network, ANN）**：模仿生物大脑结构的计算模型，由输入层、隐藏层和输出层组成。
- **卷积神经网络（Convolutional Neural Network, CNN）**：主要应用于图像处理，通过卷积层识别局部特征，并通过池化层减少空间维度。
- **循环神经网络（Recurrent Neural Network, RNN）**：适用于处理序列数据，如时间序列或自然语言，通过在隐藏层中引入循环连接保持状态信息。
- **长短期记忆网络（Long Short-Term Memory, LSTM）**：RNN 的变体，通过引入门控机制解决长期依赖问题，允许网络记住重要信息。
- **门控循环单元（Gated Recurrent Unit, GRU）**：LSTM 的简化版本，减少门控机制数量，同时保持对长期依赖的有效处理能力。
- **自动编码器（Autoencoder）**：无监督学习技术，用于学习高效编码，由编码器和解码器组成。
- **生成对抗网络（Generative Adversarial Network, GAN）**：由生成器和判别器两部分组成，通过对抗训练生成器学会生成逼真样本，判别器学会区分真伪。
- **Transformer**：使用自注意力机制的模型，用于处理序列数据，消除了 RNN 中的顺序依赖性，允许并行处理。
- **多层感知器（Multilayer Perceptron, MLP）**：具有至少三层的全连接神经网络，用于分类或回归任务。
- **激活函数（Activation Function）**：为神经网络增加非线性，常见激活函数包括 ReLU、sigmoid 等。
- **预训练模型（Pre - trained Model）**：在大规模数据上预先训练好的模型，可以在特定任务上进行微调（Fine - tuning），提高模型性能和开发效率。
- **迁移学习（Transfer Learning）**：将在一个任务上训练好的模型参数迁移到另一个相关任务上，以减少训练时间和数据需求。
- **强化学习中的Q - 学习（Q - Learning）**：一种无模型的强化学习算法，通过学习状态 - 动作对的价值（Q - value）来选择最优动作。
- **深度强化学习（Deep Reinforcement Learning）**：结合深度学习和强化学习的技术，使用神经网络来近似价值函数或策略函数，如AlphaGo就是基于此技术。
- **模型压缩（Model Compression）**：通过量化、剪枝等技术减小模型的大小和计算复杂度，使其更适合在移动设备或资源受限的环境中部署。

## 强化学习（RL）
- **智能体（Agent）**：在环境中学习和行动的实体。
- **环境（Environment）**：智能体与之交互的世界。
- **奖励（Reward）**：智能体根据行动获得的反馈。
- **策略（Policy）**：智能体决定行动的规则。
- **Q-Learning**：一种无模型强化学习方法。
- **Deep Q-Network（DQN）**：结合深度学习的 Q-Learning 方法。

## 自然语言处理（Natural Language Processing, NLP）
- **词嵌入（Word Embedding）**：将词汇映射到向量空间的技术，使词汇之间的相似性在数学上得到表达。
- **词干提取（Stemming）**：将单词减少到其词根形式的过程，有助于减少词汇的数量。
- **命名实体识别（Named Entity Recognition, NER）**：从文本中识别出实体（如人名、地名）的任务。
- **情感分析（Sentiment Analysis）**：分析文本中的情绪倾向，通常用于社交媒体监控、市场研究等领域。
- **主题建模（Topic Modeling）**：从文档集中识别出主题的过程，常用于文档分类、信息检索等领域。
- **语义分析（Semantic Analysis）**：理解句子的意义，包括词语意义、句子意义等层次。
- **句法分析（Syntactic Analysis）**：分析句子的语法结构，确定句子成分之间的关系。
- **机器翻译（Machine Translation）**：将文本从一种语言翻译成另一种语言的任务。
- **问答系统（Question Answering System）**：根据问题提供基于文本的答案的系统。
- **文本摘要（Text Summarization）**：自动生成文本的总结，通常包括提取式摘要和生成式摘要两种方法。
- **分词（Tokenization）**：把文本拆分成最小的词或子词单元。
- **嵌入（Embedding）：将文本转换成向量（如 Word2Vec、BERT embedding）。
- **注意力机制（Attention）：让模型关注重要的输入部分（Transformer 关键技术）。
- **文本生成（Text Generation）：如 GPT 生成文章。
- **词元（token）**：在机器学习中，代表模型处理的基本单位，如文本中的单词或子词。
- **词元化器（tokenizer）**：将文本或其他模态数据转换为词元的过程或工具。
- **嵌入（Embedding）**：将数据转换为向量表示的技术，便于模型进行语义理解和计算。
- **有监督微调（Supervised Fine-Tuning，SFT）**：在预训练模型基础上使用有标记数据进行微调以提高性能。
- **零样本学习（zero-shot）**：无需额外标注数据，直接使用预训练模型完成新任务的学习方式。
- **少样本学习（few-shot）**：使用少量样本对预训练模型进行微调的学习方法。
- **基于人类反馈的强化学习（Reinforcement Learning from Human Feedback，RLHF）**：通过人类反馈指导模型行为的强化学习方法。

## 数据相关
- **训练集（Training Set）**：用于训练模型的数据集，模型通过学习这些数据来提取特征和规律。
- **验证集（Validation Set）**：用于调整模型超参数和评估模型性能的数据集，帮助防止模型过拟合。
- **测试集（Test Set）**：用于最终评估模型性能的数据集，模型在测试集上的表现可以反映其在实际应用中的泛化能力。
- **数据标注（Data Annotation）**：对数据进行标记和注释的过程，如在图像中框出目标物体、为文本数据标注情感类别等，是监督学习中必不可少的步骤。
- **数据增强（Data Augmentation）**：通过对原始数据进行变换（如旋转、缩放、裁剪等）生成更多样的数据，增加数据集的多样性，提高模型的泛化能力。
- **特征工程（Feature Engineering）**：从原始数据中提取、选择和构造有用的特征，以提高模型的性能，是机器学习中非常重要的环节。
- **过拟合（Overfitting）**：模型在训练集上表现很好，但在测试集或新数据上表现较差的现象，通常是由于模型过于复杂或数据不足导致的。
- **欠拟合（Underfitting）**：模型在训练集上表现不佳，无法很好地学习数据中的规律，通常是由于模型过于简单或数据质量差导致的。
- **数据预处理（Data Preprocessing）**：对数据进行清洗、归一化、标准化等处理，使其更适合用于模型训练。
- **数据集（Dataset）**：用于训练、验证和测试模型的数据集合，常见的数据集有ImageNet（图像数据集）、MNIST（手写数字数据集）等。

## 计算机视觉（Computer Vision）
- **图像分割（Image Segmentation）**：将图像分成多个部分或区域，每个区域具有相似的属性。像素级分类，如语义分割、实例分割。
- **目标检测（Object Detection）**：在图像中识别并定位多个目标的任务。检测图像中的物体及其位置。
- **图像分类（Image Classification）**：根据图像内容对其进行分类的任务。
- **图像生成（Image Generation）**：创建新的图像的任务，常用方法包括 GANs 等。
- **人脸识别（Face Recognition）**：从图像中识别个体身份的过程，通常包括人脸检测和身份验证两个步骤。
- **光学字符识别（Optical Character Recognition，OCR）**：从图像中提取文字。
- **风格迁移（Style Transfer）**：把一张图片的风格应用到另一张图片上。

## 工程与部署
- **模型训练（Model Training）**：使用数据集使模型适应特定任务的过程。
- **模型评估（Model Evaluation）**：测量模型性能的标准和方法，包括精度、召回率等指标。
- **模型优化（Model Optimization）**：改进模型以提高效率或效果的技术，如剪枝、量化等。
- **模型压缩（Model Compression）**：减少模型大小以适应有限资源环境的技术。
- **模型融合（Model Ensemble）**：将多个模型的预测结果结合起来以提高性能的技术。
- **持续集成 / 持续交付（Continuous Integration/Continuous Delivery, CI/CD）**：软件开发的自动化流程，确保代码变更可以快速可靠地部署到生产环境中。
- **容器化（Containerization）**：使用容器来打包和运行应用程序的技术，便于环境的一致性和可移植性。
- **微服务架构（Microservices Architecture）**：将应用程序分解为小的服务，每个服务都可以独立开发和部署。
- **API 设计（API Design）**：构建应用程序接口的过程，确保接口易于使用且功能强大。
- **边缘计算（Edge Computing）**：在数据源附近处理数据而不是发送到云端的技术，以降低延迟和带宽消耗。
- **偏差（Bias）**：模型预测与真实值之间的系统性误差。
- **方差（Variance）**：模型对数据变化的敏感程度，高方差可能导致过拟合。
- **梯度下降（Gradient Descent）**：用于优化模型的算法。
- **反向传播（Backpropagation）**：神经网络训练时的核心方法。
- **训练轮数（Epoch）**：模型遍历数据集的次数。
- **批量大小（Batch Size）**：每次训练时处理的数据量。
- **微调（Fine-tuning）**：在预训练模型的基础上优化特定任务。
- **每秒浮点数运算次数（Floating-point Operations Per Second，FLOPS）**：衡量计算设备浮点运算能力的指标。
- **百万次浮点运算（Million FLOPS，MFLOPS）**：每秒百万次浮点运算的算力单位。
- **十亿次浮点运算（Giga FLOPS，GFLOPS）**：每秒十亿次浮点运算的算力单位。
- **图形处理器（Graphics Processing Unit，GPU）**：一种专门用于加速图形渲染和并行计算的处理器，广泛应用于深度学习领域。
- **计算统一设备架构（Compute Unified Device Architecture，CUDA）**：英伟达推出的一种用于加速图形运算和深度学习运算的软件架构。
- **Transformer模型**：基于注意力机制的神经网络模型架构，2017年由Google团队提出，对深度学习的发展具有重要意义。
- **模型蒸馏（蒸馏）**：一种模型压缩技术，通过训练较小的学生模型来模仿较大的教师模型。
- **去噪扩散概率模型（Denoising Diffusion Probabilistic Models，DDPM）**：文生图技术的基础之一。
- **对比语言图像预训练（Contrastive Language-Image Pre-training，CLIP）**：文本和图像对齐的技术，是文生图算法的重要基础。
- **扩散Transformer（Diffusion Transformer，DiT）**：当前文生图和文生视频技术的主流模型架构。
- **低秩适应（Low-Rank Adaptation/Adaptor，LoRA）**：一种大模型的微调手段，用于控制图像生成的风格等。
- **控制网络（ControlNet）**：用于精确控制模型输出的网络，多用于文生图领域。
- **过拟合（Overfitting）**：模型在训练数据上表现良好但在新数据上表现不佳的现象。
- **欠拟合（Underfitting）**：模型未能充分学习训练数据的特征，导致性能不佳。
- **容器化技术**：将应用程序及其依赖项打包到轻量级、可移植的容器中的软件开发实践。
- **异构计算**：利用不同架构的计算资源协同工作的计算方式，常见于人工智能领域。
- **弹性计算**：一种云计算服务，允许用户根据需求动态调整计算资源。
- **微服务**：一种软件开发架构，将应用程序分解为一组独立的小服务，提高开发和部署的灵活性。
- **分布式集群、分布式计算**：通过多个计算节点协同工作，提高计算能力和资源利用率。

## 行业与应用
- **智能语音助手（Intelligent Voice Assistant）**：如Siri、Alexa、小爱同学等，通过语音识别和自然语言处理技术为用户提供语音交互服务。
- **医疗影像分析（Medical Image Analysis）**：使用 AI 辅助医生进行疾病诊断的技术。
- **自动驾驶（Autonomous Driving）**：利用 AI 实现车辆自主驾驶的技术，涉及感知、规划、控制等多个方面。
- **智能推荐系统（Intelligent Recommendation Systems）**：向用户提供个性化建议的系统，广泛应用于电子商务、社交媒体等领域。
- **金融科技（Fintech）**：在金融领域应用 AI 提高效率的技术，包括风险管理、交易执行等。
- **智能客服（Smart Customer Service）**：使用聊天机器人提供客户服务的技术，能够 24 小时不间断响应客户查询。
- **物联网（Internet of Things, IoT）**：连接物理设备并通过互联网交换数据的技术，涉及传感器、云计算等多个方面。
- **增强现实（Augmented Reality, AR）**：在现实世界中叠加数字信息的技术，用于游戏、教育等领域。
- **虚拟现实（Virtual Reality, VR）**：创建完全沉浸式的数字环境的技术，广泛应用于娱乐、培训等领域。
- **游戏 AI（Game AI）**：在游戏中模拟玩家或其他非玩家角色的行为的技术。
- **语音识别（Speech Recognition）**：将口语转换为文本的技术，广泛应用于语音助手、电话会议等领域。

## 软件与工具
- **TensorFlow**：由 Google 开发的开源机器学习框架，支持多种机器学习任务。
- **PyTorch**：由 Facebook AI 实验室开发的开源机器学习库，广泛用于研究和生产环境中。
- **Scikit-learn**：Python 中的机器学习库，提供了各种监督和无监督学习算法。
- **Keras**：用于构建和训练深度学习模型的高级 API，支持 TensorFlow 等多种后端。
- **Pandas**：Python 中的数据分析库，提供了数据结构和数据操作工具。
- **NumPy**：Python 中的科学计算包，提供了数组操作和支持向量运算的功能。
- **Matplotlib**：Python 中的绘图库，支持多种图表类型，便于数据可视化。
- **Jupyter Notebook**：可用于编写和运行代码的 Web 应用程序，支持多种编程语言。
- **Docker**：开源平台，用于创建、部署和管理应用程序的容器。
- **Git**：分布式版本控制系统，用于跟踪项目历史记录和协同开发。
- **Langchain**：开源的大语言模型训练和应用框架，旨在提高大模型应用开发的效率。
- **Llama Index**：与Langchain类似的大模型开发解决方案或技术架构。
- **ChatGPT**：OpenAI公司推出的一款基于GPT-3.5预训练模型的通用智能对话助手。
- **ComfyUI**：用于与AI生成模型配合使用的用户界面框架，提供更舒适的交互体验。

## 伦理与隐私
- **AI伦理（AI Ethics）**：研究AI技术对人类社会、道德和价值观的影响，包括隐私保护、算法公平性、人工智能的责任等问题。
- **算法偏见（Algorithmic Bias）**：由于训练数据的问题导致的模型不公平行为。
- **透明度（Transparency）**：模型决策过程的可解释性和清晰度，对于建立信任非常重要。
- **隐私保护（Privacy Protection）**：在收集和使用个人数据时确保用户隐私的技术和方法。
- **数据安全（Data Security）**：防止数据泄露或未经授权访问的技术和实践。
- **公平性（Fairness）**：确保算法不对任何群体产生不利影响的原则。
- **人工智能的可解释性（Interpretability of AI）**：使AI模型的决策过程和结果能够被人类理解和解释，这对于提高用户对AI的信任和应用安全性非常重要。
- **人工智能的可持续性（Sustainability of AI）**：关注AI技术对环境的影响，如训练和运行AI模型所需的能源消耗，以及如何通过技术优化实现可持续发展。
- **人工智能的法律与监管（AI Law and Regulation）**：制定相关的法律法规和监管政策，规范AI技术的研发、应用和市场行为，确保其符合社会。

## 法律与合规
- **GDPR**：欧盟关于个人数据保护的规定，要求企业遵守严格的隐私保护措施。
- **版权法（Copyright Law）**：保护创作者对其作品的权利，包括复制、发行、展示等权利。
- **专利法（Patent Law）**：保护发明者对其发明的权利，授予发明者一段时间内的独家使用权。
- **知识产权（Intellectual Property, IP）**：包括专利、商标、版权和工业设计权等，保护创新者的创造性工作。
- **数据主权（Data Sovereignty）**：数据存储和使用的国家法律，强调数据的地域管辖权。

## 商业与市场
- **价值主张（Value Proposition）**：产品或服务提供的独特好处，用来吸引目标客户群。
- **市场细分（Market Segmentation）**：将市场分为具有共同需求的小群体的过程，便于针对性营销。
- **竞争分析（Competitive Analysis）**：评估竞争对手的优势和劣势，以制定有效的市场策略。
- **商业模式（Business Model）**：组织如何创造、传递和捕获价值的描述，包括收入来源、成本结构等。

