# AI大模型最新发布[update@202503]

[图]

## OpenAI
**GPT-4o**：多模态，“o”代表Omni，即全能的意思，凸显了其多功能的特性。
- 多模态交互，GPT-4o可以接受文本、音频和图像的任意组合作为输入，并生成文本、音频和图像的任意组合输出。
- 实时推理能力，多语言支持，情绪理解，快速响应（GPT-4o的平均响应时间达到了320毫秒）。
- 采用单一的Transformer架构进行设计，将文本、图像和音频等不同模态的数据统一到一个神经网络中处理。
- gpt-4o-transcribe (语音转文本)，gpt-4o-mini-transcribe (语音转文本)，gpt-4o-mini-tts (文本转语音)。

## Google
**Gemini 2.5**：被定位为一款“思考型模型”，将推理能力直接嵌入了模型中，它能够在回答问题前先进行思考分析，从而提供更准确、更深入的回答。能处理来自文本、音频、图像、视频和大型数据集的输入，甚至能够理解整个代码仓库的结构和内容。初始版本支持100万token的上下文长度，并将很快升级至200万token，这是目前Gemini实验模型中最大的上下文窗口之一。
**Gemma3**：轻量级、高性能的开源多模态AI模型，基于Gemini 2.0技术构建，专为在单GPU或TPU上运行而设计。
- 模型规模：1 亿到 270 亿参数，共四个版本：1B、4B、12B、27B。提供不同标准格式的量化版本，包括每通道 int4、每块 int4 和切换 fp8。
- 基于 Transformer 的解码器专用架构，继承自 Gemma 2，并进行多项改进。
- 多模态，上下文长度1B 支持32K，其余模型支持128K。
- 训练方法：使用知识蒸馏进行预训练，并采用改进的后训练方法进行指令微调。
- Gemma 3系列模型包含4个版本，每个版本均开源了预训练基座版本（pt后缀版本，表示pre-training）和指令微调后的版本（it后缀版本，表示instruction fine-tuned），也就是说共开源了8个版本的大模型。而最大参数规模的Gemma 3-27B IT的fp16精度大小为54.8GB，int8量化后27GB，两张4090可用，INT4量化后需要14GB显存，单张4090完全没问题。而这个版本的模型评测结果非常好，在大模型匿名竞技场（Chatbot Arena）上得分1338分（截止2025年3月8日），排名全球第9，仅次于o1-2024-12-17模型，超过了Qwen2.5-Max以及DeepSeek V3等。
- 模型地址：[https://ollama.com/library/gemma3](https://ollama.com/library/gemma3)


## 阿里Qwen
**Qwen2.5-Omni**：多模态大模型，全面的多模式感知设计，可以无缝处理包括文本、图像、音频和视频的各种输入，同时支持流式的文本生成和自然语音合成输出。团队还提出了一种名为 TMRoPE（Time-aligned Multimodal RoPE）的新型位置嵌入，用于同步视频输入与音频的时间戳。
- 多模态能力：作为端到端多模态旗舰模型，Qwen2.5-Omni能够处理文本、图像、音频和视频等多种输入形式，并支持实时流式响应生成文本和自然语音合成输出。
- 架构创新：采用了独特的Thinker-Talker双核架构，Thinker模块负责处理多模态输入，生成高层语义表征及文本内容；Talker模块则负责将语义表征与文本转化为流畅的语音输出。
- 开源与应用：Qwen2.5-Omni-7B已开源，采用Apache 2.0许可证，可在Hugging Face、ModelScope、DashScope和GitHub上找到，便于开发者和企业在终端智能硬件上部署
- GitHub：[https://github.com/QwenLM/Qwen2.5-Omni](https://github.com/QwenLM/Qwen2.5-Omni)
- Hugging Face：[https://huggingface.co/Qwen/Qwen2.5-Omni-7B](https://huggingface.co/Qwen/Qwen2.5-Omni-7B)
- ModelScope：[https://modelscope.cn/models/Qwen/Qwen2.5-Omni-7B](https://modelscope.cn/models/Qwen/Qwen2.5-Omni-7B)

**Qwen2.5-Max**：采用了超大规模 MoE 架构，该架构通过动态选择合适的“专家”模型来优化计算资源，显著提高了推理速度和效率。此外，该模型在长上下文处理方面取得了突破性进展，支持高达100万 token 的上下文窗口，成为业内首个达到此规模的公开可用模型。通过稀疏注意力机制，Qwen2.5-Max 在处理百万 token 输入时的速度比传统方法快3到7倍。
- 性能表现：Qwen2.5-Max凭借超过20万亿tokens的预训练数据量，在知识、编程、综合能力等主流权威基准测试中展现出全球领先的性能，甚至超越了GPT-4o等模型。
- 应用与获取：企业和开发者可通过阿里云百炼平台调用新模型API，或在Qwen Chat平台上直接与模型对话。

## DeepSeek
**DeepSeek-V3-0324**：
- 性能提升：DeepSeek-V3-0324在多个关键性能指标上实现了显著提升，包括推理效率、代码生成能力、中文写作流畅度以及搜索功能的优化。在与业界知名竞品如Claude-3.7-Sonnet、Qwen-Max的对比测试中，展现出了全面的领先优势，尤其在涉及数学运算和代码处理的专业评测集上，甚至超越了GPT-4.5。
- 多模态能力：新增了“多模态引擎”功能，可以同时解析图文信息，以html格式输出图片。
- 模型参数：开源版本模型体积为6850亿参数。
- 开源与应用：DeepSeek-V3-0324的开源版本已上线Hugging Face，并迅速登上了Trending榜单。
- 硬件适配：摩尔线程GPU成功完成了对DeepSeek-V3-0324的无缝适配与升级，展现了高效的技术支持与响应速度。

**DeepSeek-R1**：
- 专家混合模型（MoE）：DeepSeek-R1采用专家混合模型架构，将模型划分为多个“专家”子网络，每个子网络都擅长处理输入数据的子集。这种架构在执行任务时，只有模型的相关部分会被激活，从而降低计算资源的消耗。
- 上下文长度：DeepSeek-R1基于DeepSeek-V3的基础模型架构构建，两者都具有128K的上下文长度，该长度通过一种称为YaRN（Yet another RoPE extensioN）的技术进行扩展，该技术扩展了LLM的上下文窗口。
- 层：DeepSeek-R1具有一个嵌入层以及61个Transformer层。前三层由创新的Multi-Head Latent Attention (MLA)层和一个标准的Feed Forward Network (FFN)层组成，而不是Transformer层上典型的多头注意力(MHA)机制。
- 多头注意力：MLA配备了低秩键值联合压缩，这在推理期间需要更少量的键值(KV)缓存，因此与传统方法相比，内存开销减少了5%到13%，并且提供了比MHA更好的性能。
- 多token预测：这是一种先进的语言建模方法，可以并行预测序列中的多个未来token，而不是一次预测一个后续单词。最初由Meta引入，多token预测(MTP)使模型能够利用多个预测路径（也称为“头”），从而可以更好地预测token表示，并提高模型在基准测试中的效率和性能。
- 推理能力：DeepSeek-R1在各种推理基准测试中表现出最先进的性能，尤其是在与数学和相关学科相关的问题中。在一些与数学相关的指标上，它被证明优于OpenAI的o1。它精通复杂的推理、问题解答和指令任务。
- 基于群体相对策略优化的强化学习：DeepSeek-R1基于之前的模型DeepSeek-V3-Base构建，采用多阶段训练，包括监督微调和基于群体相对策略优化的强化学习。GRPO专为增强推理能力和降低计算开销而设计，它无需外部“评论家”模型；而是相对评估各组响应。
- 思维链：DeepSeek-R1使用思维链(CoT)提示来处理推理任务并进行自我评估。这通过指导模型以结构化的方式分解复杂问题来模拟类人的推理，从而使其能够逻辑地推导出连贯的答案，并最终提高其答案的可读性。
- 蒸馏：使用精选的数据集，DeepSeek-R1已被蒸馏成更小、更开放的版本，这些版本性能相对较高，但运行成本更低，最值得注意的是使用了Qwen和Llama架构。
